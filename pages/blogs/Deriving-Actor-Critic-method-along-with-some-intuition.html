 <html>
 <head>
   <meta charset="utf-8">
  <meta name="viewport" content="width=device-width">
    <script>
	window.MathJax = {
  tex: {
    tags: 'ams'
  }
};
  </script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
 <style>
.wrap-collabsible {
  margin-bottom: 1.2rem 0;
}

input[type='checkbox'] {
  display: none;
}

.lbl-toggle {
  display: block;

  font-weight: none;
  font-family: monospace;
  font-size: none;
  text-transform: none;
  text-align: none;

  padding: 1rem;

  color: #A77B0E;
  background: none;

  cursor: pointer;

  border-radius: 7px;
  transition: all 0.25s ease-out;
}

.lbl-toggle:hover {
  color: #7C5A0B;
}

.lbl-toggle::before {
  content: ' ';
  display: inline-block;

  border-top: 5px solid transparent;
  border-bottom: 5px solid transparent;
  border-left: 5px solid currentColor;
  vertical-align: middle;
  margin-right: .7rem;
  transform: translateY(-2px);

  transition: transform .2s ease-out;
}

.toggle:checked + .lbl-toggle::before {
  transform: rotate(90deg) translateX(-3px);
}

.collapsible-content {
  max-height: 0px;
  overflow: hidden;
  transition: max-height .25s ease-in-out;
}

.toggle:checked + .lbl-toggle + .collapsible-content {
  max-height: 100vh;
}

.toggle:checked + .lbl-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}

.collapsible-content .content-inner {
  background: rgba(250, 224, 66, .2);
  border-bottom: 1px solid rgba(250, 224, 66, .45);
  border-bottom-left-radius: 7px;
  border-bottom-right-radius: 7px;
  padding: .5rem 1rem;
}


.content {
  max-width: 750px;
  margin: auto;
  line-height: 1.8;
}
.pages {
 margin-left: auto;
}
/* When it is unvisited link */  
a:link {
color: #7393B3;
}

/* When it is visited link */
a:visited {
color: #7393B3;
}

/* When you mouse over link */
a:hover {
color: red;
}

/* When it is selected link */
a:active {
color: blue;
}
h2 {

margin-bottom: 5px;

}
div.box {
  border: none;
  outline: 2px solid black;
  margin: 35px;
}
</style>
</head>



<body text="#36454F">
<div align="middle" class="pages">
<p>Rohan Thorat</p>   
<a style="text-decoration:none;" href="https://rohan-v-thorat.github.io/" > About </a> |
<a style="text-decoration:none;" href="/pages/blogs.html" > Blog </a> |
<a style="text-decoration:none;" href="https://github.com/rohan-v-thorat" target="_blank"> GitHub </a> 
 <a style="text-decoration:none;" href="/pages/publications.html" > Publications </a> |
<a style="text-decoration:none;" href="/pages/teaching.html" > Teaching </a> |
<a style="text-decoration:none;" href="/pages/extra-activities.html" > Extra-activities </a>
</div>
<hr>
<div class="content">
 
<h1 style="color:#595959;line-height: 1.5"><b>Deriving Actor-Critic method along with some intuition  </b></h1>

<p style="color:#818589;font-family: cursive">Posted on 28<sup>th</sup> October, 2023</p>
Pre-requisite: Basics of Reinforcement Learning (RL)<br>
<div align ="middle" class="content">
  <img src="blogs-images/RL-dog-human-mod.gif" style="width:65%"> 
</div>
 
<p style="color:#A9A9A9">Please feel free to skip any of the content if you know it aprior </p>
<h4 style="color:#666666;margin-bottom: -20px">Content:</h4>
 <ul>
  <li><a href="#Intro">Introduction</a></li>
  <li><a href="#CF">Classification of reinforcement learning methods </a></li>  
  <li><a href="#PGM">Monte-Carlo Policy-Gradient Method (REINFORCE)</a></li>
  <li><a href="#VRT">Variance reduction technique</a></li>
  <ul>
  	<li><a href="#RTG">Policy gradient with reward-to-go</a></li>
  	<ul>
  	     <li><a href="#RTG_bias">No-Bias proof</a></li>
  	     <li><a href="#RTG_variance">Variance reduction proof (not done)</a></li>
  	</ul>
  	<li><a href="#BL">Policy gradient with baseline</a></li>
  	<ul>
  	     <li><a href="#BL_bias">No-Bias proof</a></li>
  	     <li><a href="#BL_variance">Baseline derivation for least variance</a></li>
  	</ul>
  </ul>
  <li><a href="#AC">Actor-Critic Method</a></li>
</ul> 

<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Introduction%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h2 id="Intro" style="color:#666666">Introduction</h2>
In reinforcement learning, Agent interacts with the environment to learn a policy. This policy can be represented as \(\pi_{\theta}\) parameterized by \(\theta\) which is a distribution over action for given state i.e \(a \sim \pi_{\theta}(s)\). The policy usually is represented by \(\pi\) as it is stochastic in nature, this stochasticity makes the agent capable of exploring the environment. We need to know few notations:
\begin{align*}
	s_i & \leftarrow \text{state of the agent at }i^{\text{th}}\text{ timestep}\\
	a_i & \leftarrow \text{action taken by the agent based on }s_i\\
	r_i & \leftarrow \text{reward at }i^{\text{th}}\text{ timestep} \\
	\tau & \leftarrow s_0,a_0,s_1,a_1,s_2,....s_T\\
	R(\tau) & \leftarrow \text{total reward} \\
	s' & \leftarrow \text{next state}\\
	s_T & \leftarrow \text{terminal state}
\end{align*}
where \(r_i=f(s_i,a_i,s_{i+1})\) <br>
As the policy is probabilistic in nature, so the trajectory/path(\(s_0,a_0,r_0,s_1,a_1,r_1,s_2,....s_T\)) followed by the agent is also probabilistic. The probability density of \(\tau\) can be expressed as:
\begin{align*}
	p(\tau) &= p(s_0,a_0,s_1,a_1,s_2......s_T) \\[2pt]
		&= p(s_0)p(a_0,s_1,a_1,s_2......s_T|s_0) \hspace{7.5em}\because p(A,B)=p(A)p(B|A) \\[2pt]
		&= p(s_0)p(a_0|s_0)p(s_1,a_1,s_2......s_T|s_0,a_0)\\[2pt]
		&= p(s_0)p(a_0|s_0)p(s_1|s_0,a_0)p(a_1,s_2......s_T|s_0,a_0,s_1)\\[2pt]
		&= p(s_0)p(a_0|s_0)p(s_1|s_0,a_0)p(a_1,s_2......s_T|s_1) \hspace{2em} \because\text{ Agent follows markov decision process}\\[2pt]
		&= p(s_0)p(a_0|s_0)p(s_1|s_0,a_0)p(a_1|s_1)p(s_2|s_1,a_1)p(a_2|s_2).....p(s_T|s_{T-1},a_{T-1})\\[2pt]
		&= p(s_0) \prod_{i=0}^{T-1} p(a_i|s_i)p(s_{i+1}|s_i,a_i) 
\end{align*}
As the policy is represented by \(\pi\); \(p(a|s)\) in the above equation can be replaced with \(\pi_\theta(a|s)\) and \(p(\tau)\) can be replaced with \(\pi'_\theta(\tau)\):
\begin{equation} \label{eq:prob_of_tau}
	\pi'_\theta(\tau) = p(s_0) \prod_{i=0}^{T-1} \pi_\theta(a_i|s_i){\underbrace{p(s_{i+1}|s_i,a_i)}_\text{transitional probability}}
\end{equation}

<p style="color:#A9A9A9">Note: \(\pi\) is distribution over \(a\) and \(\pi'\) is distribution over \(\tau\) whereas \(\pi(a|s)\) is probability density of \(a\) given \(s\) and \(\pi'(\tau)\) is probability density of \(\tau\) </p>

<div class="wrap-collabsible">
  <input id="collapsible1" class="toggle" type="checkbox">
  <label for="collapsible1" class="lbl-toggle">Why \(\tau \neq s_0,a_0,r_0,s_1,a_1,r_1,\cdots,s_T\) ?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
       \(R(\tau)\) is the total reward obtained by the agent in one complete trajectory.
       \begin{align*}
       R(\tau) &= r_0(s_0,a_0,s_1)+r_1(s_1,a_1,s_2)+\cdots+r_{T-1}(s_{T-1},a_{T-1},s_T)\\
       		&= \sum_{i=0}^{T-1}r_i(s_i,a_i,s_{i+1})\\
       \therefore \tau &= s_0,a_0,s_1,a_1,s_2,\cdots,s_T		
       \end{align*} 
       So we can say that \(\tau\) represent only some part of trajectory which is state's and action's.
      </p>
    </div>
  </div>
</div> 
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Classification%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h2 id="CF" style="color:#666666">Classification of reinforcement learning methods</h2>
<div align ="middle" class="content">
  <img src="blogs-images/classification.png" style="width:65%"> 
</div>
<u>Value-based method</u>:
<p style=" margin-left: 60pt"> In this method, state-action value function \(Q(s,a)\) is used by the agent to decide which action to take. \(Q(s,a)\) is expected total reward from the current state \(s\) and action \(a\) upto the agent's terminal state<p/>
\begin{equation*}
	a = \arg\max_{a} Q(s,a)	
\end{equation*}

<u>Policy-based method</u>:
<p style=" margin-left: 60pt"> In this method, agent take the action directly based on state with the help of policy function which maps state to action.<p/>
\begin{equation*}
	a = f(s)
\end{equation*}
<p style="color:#A9A9A9">Note: RL methods can also be classified in other ways but it easy to understand RL if we prefer to classify it as value-based and policy-based.</p>
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Policy-gradient method%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h2 id="PGM" style="color:#666666">Monte-Carlo Policy-Gradient Method (REINFORCE)</h2>
Every problem in reinforcement learning is approached with a intention of finding such a policy which can make agent to be able to follow the best possible trajectory. To do so, we need to first set the objective function accordingly:
\begin{align}
	J(\theta) &=  \displaystyle \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[R(\tau)]
\end{align}
Here \(R(\tau)\) is total reward gained by the agent over the complete trajectory. To maximize the \(J(\theta)\) , the \(\theta\) need to be updated using gradient ascent approach:
\begin{equation*} 
	   \theta = \theta + \alpha \nabla_\theta J(\theta)
\end{equation*}
Now lets focus on solving for \(\nabla_\theta J(\theta)\)
\begin{align}
	\nabla_\theta J(\theta) =& \nabla_\theta \displaystyle \mathop{\mathbb{E}}_{\tau \sim \pi'_{\theta}}[R(\tau)] \label{J_grad} \\
	=& \nabla_\theta \int \pi'_\theta(\tau) R(\tau) d\tau \notag
\end{align}
Here we can not do the integration as \(\pi'_\theta(\tau)\) and \(R(\tau)\) both are unknown to us neither we can do monte-carlo approximation of equation \ref{J_grad} because our equation is not in \(\displaystyle \mathop{\mathbb{E}}\)[] term. It has one extra term which is gradient operator outside of \(\mathop{\mathbb{E}}\)[]. So we need to somehow, shift the gradient operator inside the \(\mathop{\mathbb{E}}\)[] in equation \ref{J_grad}.<br>
<div class="wrap-collabsible">
  <input id="collapsible2" class="toggle" type="checkbox">
  <label for="collapsible2" class="lbl-toggle">Why \(\pi'_\theta(\tau)\) is unknown?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
        From equation \ref{eq:prob_of_tau}, we have <br>
        $$\pi'_\theta(\tau) = p(s_0) \prod_{i=0}^{T-1} \pi_\theta(a_i|s_i)p(s_{i+1}|s_i,a_i)$$
        In this equation, we know policy i.e \(\pi_\theta(a_i|s_i)\) but other two terms: \(p(s_0)\), \(p(s_{i+1}|s_i,a_i)\) is not known to us because we don't know the mathematical model of the environment. If we knew the mathematical model of environment then there would be no need to use reinforcement learning method in first place. 
      </p>
    </div>
  </div>
</div> 
\begin{align*}
	\nabla_\theta J(\theta) =& \nabla_\theta \int \pi'_\theta(\tau) R(\tau) d\tau\\
				=&  \int \nabla_\theta(\pi'_\theta(\tau)) R(\tau) d\tau \hspace{2em}\because \nabla_\theta\text{ is a linear operator}
\end{align*}
To transform this integral into \(\mathop{\mathbb{E}}\)[] from, we need atleast one \(\pi'_\theta(\tau)\) with no gradient operation over it in the above equation.
\begin{align*}
	\nabla_\theta J(\theta) =& \int \frac{\pi'_\theta(\tau)}{\pi'_\theta(\tau)} \nabla_\theta(\pi'_\theta(\tau)) R(\tau) d\tau\\
	=& \int \pi'_\theta(\tau) \frac{\nabla_\theta(\pi'_\theta(\tau))}{\pi'_\theta(\tau)} R(\tau) d\tau\\
	=& \int \pi'_\theta(\tau) \bigg[ \nabla_\theta \log (\pi'_\theta(\tau)) R(\tau) \bigg] d\tau \hspace{2em}\because \nabla_x\log f(x)= \frac{\nabla_x(f(x))}{f(x)}\\
	=& \mathop{\mathbb{E}}_{\tau \sim \pi'_{\theta}}[\nabla_\theta \log (\pi'_\theta(\tau)) R(\tau)]
\end{align*}
Cheers we did it !!! We finally were able to write \(\nabla_\theta J(\theta)\) in terms of \(\mathop{\mathbb{E}}\)[], now we can do monte-carlo approximation.
\begin{equation*}
	\nabla_\theta J(\theta) \approx \sum_{j=1}^{N} \nabla_\theta \log (\pi'_\theta(\tau_j)) R(\tau_j)
\end{equation*}
Okay, so we have successfully approximated the integration but we have more work to do. Now we have other problem, \(\pi^{'}_\theta(\tau)\) is not known to us so we can not take gradient of it. Lets replace \(\pi'_\theta(\tau)\) by equation \ref{eq:prob_of_tau}
\begin{align*}
	\nabla_\theta J(\theta) =& \sum_{j=1}^{N} \nabla_\theta \log (\pi'_\theta(\tau_j)) R(\tau_j)\\
				=& \sum_{j=1}^{N} \nabla_\theta \big[\log( p(s_{0,j}) \prod_{i=0}^{T-1} \pi_\theta(a_{i,j}|s_{i,j})p(s_{i+1,j}|s_{i,j},a_{i,j}) )\big]R(\tau_j)\\
				=& \sum_{j=1}^{N} \nabla_\theta \big[\underbrace{\log( p(s_{0,j}))}_\text{do not depend on \(\theta\)}+\; \log( \prod_{i=0}^{T-1} \pi_\theta(a_{i,j}|s_{i,j}))\;+\;\underbrace{\log(\prod_{i=0}^{T-1} p(s_{i+1,j}|s_{i,j},a_{i,j})}_\text{do not depend on \(\theta\)} )\big]R(\tau_j)\\
				=& \sum_{j=1}^{N}  \big[\cancelto{0}{\nabla_\theta \log(p(s_{0,j}))}+\; \nabla_\theta\log( \prod_{i=0}^{T-1} \pi_\theta(a_{i,j}|s_{i,j}))\;+\;\cancelto{0}{\nabla_\theta\log(\prod_{i=0}^{T-1} p(s_{i+1,j}|s_{i,j},a_{i,j}) )}\big]R(\tau_j)\\
	=& \sum_{j=1}^{N} \big[\nabla_\theta \log(\prod_{i=0}^{T-1} \pi_\theta(a_{i,j}|s_{i,j}))\big]R(\tau_j)\\
	=& \sum_{j=1}^{N} \big[\nabla_\theta \sum_{i=0}^{T-1}\log( \pi_\theta(a_{i,j}|s_{i,j}))\big]R(\tau_j)\\
	=& \sum_{j=1}^{N} \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i,j}|s_{i,j}))R(\tau_j)\\
\end{align*}
Now the above equation can be used for updating policy parameter.
<!--{write about how now we know \(R(\tau)\) but was unknown earlier}-->

<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Variance reduction technique%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h2 id="VRT" style="color:#666666">Variance reduction technique</h2>
REINFORCE idea was originated in 1992 in <a style="text-decoration:none;" href="https://link.springer.com/article/10.1007/BF00992696" target="_blank">Ronald J. Williams</a> paper. The algorithm has showed the way to solve the RL problem but has some issue. The updatation of policy parameter \(\theta\) is not gradual, it has lot of variance. This makes it difficult for the agent to find the optimum policy. To deal with the issue two effective variance reduction technique can be used.<br>
1) <i>Policy gradient with reward-to-go</i><br>
2) <i>Policy gradient with baseline</i>
<p>
we will understand both the technique intuituively as well as with it's mathematical proof.</p>
<div class="wrap-collabsible">
  <input id="collapsible3" class="toggle" type="checkbox">
  <label for="collapsible3" class="lbl-toggle">Why are we facing variance problem?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
The variance issue arised because we have approximated the \(\mathop{\mathbb{E}}_{x \sim p(x)}[f(x)]\). One of the solution to reduce the variance is to use more no. of samples while doing Monte-Carlo approximation. But using more no. of samples means it would need more compute power which is not always a feasible option. Instead one could replace \(f(x)\) with some other function such that it needs less no. of sample to approximate its \(\mathop{\mathbb{E}}\)[]. This approach is called Variance reduction technique. 
      </p>
    </div>
  </div>
</div> 
Lets write the \(\nabla_\theta J(\theta)\) equation in terms of \(\mathop{\mathbb{E}}\)[] (we will need it in this form for mathematical proof in next subsection):
\begin{equation} \label{eq:Grad_J_Exp}
	\nabla_\theta J(\theta) = \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R(\tau)\big]
\end{equation}
<div class="box">
<p style="margin:10pt;">
\(\\[10pt]\)
\(\textbf{lemma: 1}\label{lemma_1}\)
\begin{equation*}
\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=1}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))\big] = 0
\end{equation*}
\(\hspace{0.5em}\textbf{proof:}\) The equation can be written as
\begin{align*}
	\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=1}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))\big] &=\mathop{\mathbb{E}}_{\tau \sim \pi'_{\theta}}[\nabla_\theta \log (\pi'_\theta(\tau))]\\
	&= \int \pi'_\theta(\tau)\nabla_\theta \log (\pi'_\theta(\tau))d\tau\\
	&= \int \cancel{\pi'_\theta(\tau)}\frac{\nabla_\theta \pi'_\theta(\tau)}{\cancel{\pi'_\theta(\tau)}}d\tau\\
	&= \int \nabla_\theta \pi'_\theta(\tau)d\tau\\
	&= \nabla_\theta \int \pi'_\theta(\tau)d\tau\\
	&= \nabla_\theta . 1 \hspace{2em}\because\int p(x)dx=1\\
	&= 0
\end{align*}
\(\\[1pt]\)
</p>
</div>
<h3 id="RTG" style="color:#666666">1) Policy gradient with reward-to-go</h3>
<h4 style="color:#666666">Intuition:</h4>
Till now we had used total reward/return in our equation. Now it's time to improve our objective by removing things which are irrelevant to us. Let's think on one thing, <i>what is the motive of agent while it takes a action?</i> Is it to gain more reward in future or is it also to affect the past reward(which is not possible)….. a big NO! So why to keep past reward in our objective function. This is were a new term reward-to-go is introduced which take into the account only the futuristic reward.
<h4 style="color:#666666">Mathematical proof: </h4>
<h4 id="RTG_bias" style="color:#666666"><i>No-Bias proof</i> </h4>
While we try to reduce the variance it is important to keep mind that this Variance reduction technique should not add any bias in our estimated \(\nabla_\theta J(\theta)\). <br>
\(\textbf{To prove:}\) 
\begin{equation*}
\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R(\tau)\big] = \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R_i\big]
\end{equation*}
\(\textbf{Proof:}\)
\begin{align}
\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R(\tau)\big] &= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(\sum_{k=0}^{T-1}r(s_k,a_k,s_{k+1}))\big]\notag\\
	&= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(\sum_{k=0}^{T-1}r(s_k,a_k,s_{k+1}))\big]\notag\\
	&= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=1}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(\sum_{k=0}^{i-1}r(s_k,a_k,s_{k+1}))\notag\\
	&\;\;\;\;+ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))\underbrace{(\sum_{k=i}^{T-1}r(s_k,a_k,s_{k+1}))}_\text{reward-to-go}\big]\notag\\
	&= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=1}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(\sum_{k=0}^{i-1}r(s_k,a_k,s_{k+1}))\big]\notag\\
	&\;\;\;\;+ \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[\sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R_i\big]  \label{RTG_bias_1}\\
	& \hspace{2em}\because\mathop{\mathbb{E}}\text{[ ] is linear operator}\notag
\end{align}
<p style=" margin-left: 60pt"> Now we need to see whether we can equate first term from Equation \ref{RTG_bias_1} to zero then our proof would be complete.<p/>
\begin{align}
	\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=1}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(\sum_{k=0}^{i-1}r(s_k,a_k,s_{k+1}))\big] &= \underbrace{\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=1}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))\big]}_\text{=0} \notag\\
	& \times\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[(\sum_{k=0}^{i-1}r(s_k,a_k,s_{k+1}))\big] \notag\\
	&=0 \hspace{2em}...\text{refer to lemma 1}\label{RTG_bias_2}
\end{align}
<p style=" margin-left: 60pt">Action taken based on current state affect state of the agent in future and the reward the agent will get it in future but it does not have any relation with the reward agent has got in the past. So action and the past reward are independent random variable.( \(\mathop{\mathbb{E}}[XY]=\mathop{\mathbb{E}}[X]\mathop{\mathbb{E}}[Y]\) when \(X\) and \(Y\) are independent random variable) <br>
From Equation \ref{RTG_bias_1} and \ref{RTG_bias_2}, it proves that replacing total reward with reward-to-go does not add bias in our estimated \(\nabla_\theta J(\theta)\) <p/>
<h4 id="RTG_variance" style="color:#666666"><i>Variance reduction proof</i> </h4>
\(\textbf{To prove:}\) 
\begin{equation*}
Var(\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R(\tau)\big]) > Var(\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R_i\big])
\end{equation*}
\(\textbf{Proof:}\)
\begin{align}
	\text{I was not able to prove this, if anyone can prove it then please share with me through e-mail :)} \notag
\end{align}
<h3 id="BL" style="color:#666666">2) Policy gradient with baseline</h3>

<h4 style="color:#666666">Intuition:</h4>
Baseline helps the agent to know whether the taken action was better than average or below average. Action taken by the agent is average ideally only when the agent start to explore similar trajectory everytime and this indicates that now there is no need to update the policy.
<h3 style="color:#666666">Mathematical proof:</h3>

<h4 id="BL_bias" style="color:#666666"><i>No-Bias proof</i> </h4>
\(\textbf{To prove:}\) 
\begin{equation*}
\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(R_i-b)\big] = \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R_i\big]
\end{equation*}
\(\textbf{Proof:}\)
\begin{align}
	\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))(R_i-b)\big] &= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))R_i\big] \notag\\
	&\;\;\;-\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))b\big] \label{BL_bias_1}\\
\end{align}
<p style=" margin-left: 60pt"> Now we need to show second term from above Equation \ref{BL_bias_1} equal to zero then our proof will be complete. The second term can be written as: <p/>
\begin{align}
	\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i}|s_{i}))b\big] &= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \nabla_\theta\log( \pi'_\theta(\tau))b\big] \notag \\
	&= \int \pi'_\theta(\tau) \nabla_\theta\log( \pi'_\theta(\tau))bd\tau \notag\\
	&= \int \cancel{\pi'_\theta(\tau)} \frac{\nabla_\theta\pi'_\theta(\tau)}{\cancel{\pi'_\theta(\tau)}}bd\tau \notag\\
	&= b\nabla_\theta \int \pi'_\theta(\tau)d\tau \notag\\
	&= b\nabla_\theta .1 \hspace{2em}\because \int p(x)dx=1\notag\\
	&= b \times 0 \notag\\
	&= 0 \label{BL_bias_2}
\end{align}
<p style=" margin-left: 60pt"> From Equation \ref{BL_bias_1} and \ref{BL_bias_2}, it proves that baseline do not add bias in our estimated \(\nabla_\theta J(\theta)\)  <p/>

<h4 id="BL_variance" style="color:#666666"><i>Baseline derivation for least variance</i> </h4>
\begin{align*}
Var[x]&=\mathop{\mathbb{E}}[x^2] + \mathop{\mathbb{E}}[x]^2\\
	\therefore Var\big[ \nabla_\theta\log( \pi_\theta'(\tau))(R(\tau)-b)\big] &= \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ (\nabla_\theta\log( \pi_\theta'(\tau))(R(\tau)-b))^2\big] \\
	&\hspace{2em}+ \underbrace{\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ \nabla_\theta\log( \pi_\theta'(\tau))(R(\tau)-b)\big]^2}_\text{=0}\\
	\text{To find best baseline, solve for }& \frac{dVar}{db}=0\text{ and let }f(\tau)=\nabla_\theta\log( \pi_\theta'(\tau))\\
	\frac{d}{db}Var\big[ f(\tau)(R(\tau)-b)\big]&= \frac{d}{db}\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}\big[ (f(\tau)(R(\tau)-b))^2\big]\\
	&= \frac{d}{db}(\cancelto{0}{\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2R(\tau)^2]}-2\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2R(\tau)b]+b^2\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2])\\
	&= -2\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2R(\tau)]+2b\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2])=0\\
	\therefore b = \frac{\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2R(\tau)]}{\mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[f(\tau)^2])}
\end{align*}



<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h2 id="AC" style="color:#666666">Actor-Critic Method</h2>
<p style="margin-left:60pt;margin-right:60pt">“Give me six hours to chop down a tree and I will spend the first four sharpening the axe.” - Abraham Lincoln</p>

The framework of this method is build on REINFORCE with baseline method. As we have understood about REINFORCE with baseline method earlier our axe is well sharpen and now it will not take much time to understand Actor-Critic method. Agent in reinforcement learning is termed as Actor and along with 'Actor' model we have one more model named 'Critic' model. This Critic model helps the agent to learn the policy smoothly and has few more advantage which we will get to know when we see it's mathematical equation's
\(\\[20pt]\)
<div align ="middle" class="content">
  <img src="blogs-images/actor-critic.jpg" style="width:65%"> 
</div>  
<br>
From above figure, we can see that the reward is not directly recieved by the agent i.e Actor but it is feed into Critic model which gives Temporal Difference (TD) error to the Actor. For now, think T.D. error as some form of modified reward. This TD error is used to update the Actor model i.e it's policy and also it is used to update Critic model. In this algorithm baseline \(b\) is taken as \(V(s_i)\) where \(V(s_i)\) is the expected total reward from state \(s_i\) upto the state \(s_T\). Let's write \(\nabla_\theta J(\theta)\) expression:  
\begin{align*}
	\nabla_\theta J(\theta) &= \sum_{j=1}^{N}\sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i,j}|s_{i,j}))(R_{i,j}-b)\\
	&= \sum_{j=1}^{N}\sum_{i=0}^{T-1}\nabla_\theta\log( \pi_\theta(a_{i,j}|s_{i,j}))(R_{i,j}-V(s_{i,j}))\\
\end{align*}
Using above equation we can not update the policy at each timestep as we need atleast one complete trajectory to know reward-to-go value. we can sort this out by approximating reward-to-go as follows:
\begin{align*}
	R_i &= r_i + \gamma r_{i+1} + \gamma^2 r_{i+2} + \cdots + \gamma^{T-1} r_{T-1} \hspace{2em}0 \leq \gamma \leq 1\\
		&= r_i + \gamma (r_{i+1} + \gamma r_{i+2} + \cdots + \gamma^{T-2} r_{T-1})\\
		&= r_i + \gamma R_{i+1} \\
		&\approx r_i + \gamma V(s_{i+1})
\end{align*}
\(\gamma\) is a discount factor which is used to give geometric weightage to reward at each timestep. Now \(\nabla_\theta J(\theta)\) can be written as:
\begin{equation*}
	\nabla_\theta J(\theta) = \nabla_\theta\log(\pi_\theta(a_{i}|s_{i}))\underbrace{(r_i + \gamma V_\phi(s_{i+1})-V_\phi(s_{i}))}_\text{TD}
\end{equation*}
This above equation do not need to know the reward across the whole trajectory. Hence, it can be used to update policy at each timestep. The T.D puts a constraint on the updatation of policy and has effects as follow:
\begin{align*}
	r_i + \gamma V_\phi(s_{i+1})>V_\phi(s_{i})&\rightarrow\text{update's \(\theta\) in the direction of estimated }\nabla_\theta \log(a|s)\\
	r_i + \gamma V_\phi(s_{i+1})=V_\phi(s_{i})&\rightarrow\text{no update; might have reached optimum solution}\\
	r_i + \gamma V_\phi(s_{i+1})< V_\phi(s_{i})&\rightarrow \text{update's \(\theta\) in the opposite direction of estimated }\nabla_\theta \log(a|s)
\end{align*}
\(\\[20pt]\)
<div align ="middle" class="content">
  <img src="blogs-images/critic.jpg" style="width:65%"> 
</div>  
<br>
As seen from the above figure when we update Critic model, it is actually the state value function \(V(s)\) which gets updated. Here state value function \(V(s)\) is parameterized by \(\phi\). Its minimization objective function is:
\begin{equation*}
	 J(\phi) = r_i + \gamma V_\phi(s_{i+1})-V_\phi(s_{i})
\end{equation*}
As the objective is to minimize the T.D error, we will update parameter of state value function by gradient descent approach:
\begin{equation*}
	\phi = \phi - \alpha \nabla_\phi J(\phi)
\end{equation*}
<p style="color:#A9A9A9">Note: I feel it is more appropriate to say just 'temporal difference' when it is feed to Actor model whereas 'temporal difference error' when it is feed into Critic model </p>
<div class="wrap-collabsible">
  <input id="collapsible4" class="toggle" type="checkbox">
  <label for="collapsible4" class="lbl-toggle">Is \(V(s)\) definition same in all RL methods?</label>
  <div class="collapsible-content">
    <div class="content-inner">
      <p>
No. State-value function \(V(s)\) definition is dependent on objective function of agent. So, if the objective function \(J(\theta)=\displaystyle \mathop{\mathbb{E}}_{\tau \sim \pi'_\theta}[R(\tau)]\) of agent is changed then it will change the definition of \(V(s)\) . For example in case of Soft-Actor-Critic algorithm, along with total reward, entropy is also considered in the objective function which eventually changes the definition of state-value function \(V(s)\).
      </p>
    </div>
  </div>
</div> 

<hr>
<b>Algorithm</b> Actor-Critic
<hr>
Initialize parameters \(s\), \(\theta\), \(\phi\) and learning rate \(\alpha_\theta\), \(\alpha_\phi\)<br>
<b>for</b> \(t=1...T\)
<p style="margin-left:20pt;margin-top:0pt">Sample the action \(a \sim \pi_\theta'(a|s)\)<br>
Sample next state \(s' \sim P(s'|s,a)\) and reward \(r = f(s,a,s')\) from environment<br>
Update: 
<p style="margin-left:70pt;margin-top:-20pt">\(\theta=\theta+\alpha_\theta\nabla_\theta\log(\pi_\theta'(a|s))(r+V_\phi(s')-V_\phi(s))\)<br>
\(\phi = \phi - \alpha_\phi\nabla_\phi(r+V_\phi(s')-V_\phi(s))\)</p>
\(\hspace{1.8em}\)<b>set</b> \(\hspace{0.5em}s\leftarrow s'\)<br>
<b>end</b></p>
<hr>
<br>
I hope you had a great time reading this blog. Have a great day ahead !!!
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%References%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<h2 style="color:#666666">References</h2> 
 <ol>
  <li>Sutton, Richard S., and Andrew G. Barto. Reinforcement learning: An introduction. MIT press, 2018.</li>
  <li>Ravichandiran, Sudharsan. Deep Reinforcement Learning with Python: Master classic RL, deep RL, distributional RL, inverse RL, and more with OpenAI Gym and TensorFlow. Packt Publishing Ltd, 2020.</li>
   <li>https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/</li>
  <li>https://spinningup.openai.com/en/latest/spinningup/rl_intro.html</li>
<li>http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_4_policy_gradient.pdf</li>
</ol> 

<br>
<br>
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Disqus comment%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->

<div id="disqus_thread"></div>
<script>

    var disqus_config = function () {
    this.page.url = "https://rohan-v-thorat.github.io/pages/blogs/Deriving-Actor-Critic-method-along-with-some-intuition.html";  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = 2; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://https-rohan-v-thorat-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>  
</div>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

<script id="dsq-count-scr" src="//https-rohan-v-thorat-github-io.disqus.com/count.js" async></script>

<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%Rough work%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%-->
<!--<h2 style="color:#666666">Rough work</h2> 

Reinforcement learning method can be classified as policy-gradient based method and value-based method. If reading the name of value-based method makes you feel value-based method do not have policy in it then hmmm you have some confusion please complete pre-requist before proceeding further…<br>
In reinforcement learning we have a agent and a environment where agent interact with the environment by taking some actions based on its current state. In actor-critic also it follows quite similar framework, here the agent is called as actor and instead of updating its policy directly based on reward it does it based on the response of the critic model. This response from the critic model is called as 'Temporal Difference'(T.D) error. But why do we need to use a T.D error instead of just using the reward response to update the policy of the Actor, the reason been that it was observed that earlier it was difficult for a solution to converge i.e to get optimal policy because of lot of variance in the update of a policy. To overcome this, REINFORCE method was been modified with the introduction of baseline in it.<br>
With a lot of variance in the updation of the policy it was very hard to converge to the solution. So total Reward was replaced with Reward-to-go.<br>
The above change improved the solution was not yet enough. To make the variance reduce significantly, baseline term was introduced in it. Lets check does it help in variance reduction mathematically.<br>
Okay, so it does reduce the variance which means it converge to a solution but a doubt came in my mind. By adding a term will it not add a bias in the obtained solution and would result in getting a converged solution but a wrong one!!! Lets see about it in mathematical framework-->

<!--[Note: You can skip the first paragraph and start from 'Brief overview of Policy-gradient method']<br>
Hello everyone, Currently I am first-year PhD student in IIT Delhi. In my MTech, I have explored alot of online articles to understand Actor-critic algorithm and found that all the needed information about AC is not present in any single article and didn't found any correct pictorical view of AC architecture on the internet. So it made me explore a lot on the internet in the same way as the agent does in RL. I have written this article which highlight the only needed points of AC and at the same time do not miss the important bridging concepts. I hope this article saves your exploration  time.<br> -->

</div>
<!--%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%--> 


</body> 

</html>
